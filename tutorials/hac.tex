\chapter{Heteroskedasticity and Autocorrelation Consistent Standard Errors}

\section{Properties of OLS estimator}

\subsection{When is OLS estimator unbiased?}

In a linear model 
\begin{equation}
\bf y=X\beta+u, 
\end{equation}
 the OLS estimator is
\begin{equation*}
\bf \hat \beta=(X'X)^{-1}X'y
\end{equation*}

Since $\bf y=X \beta+u$,

\begin{equation}
\bf \hat \beta=\beta + (X'X)^{-1}X'u.
\end{equation}

This makes
\begin{equation}
{\rm E} \bf (\hat \beta)=\beta + (X'X)^{-1}X'{\rm E} (u|X).
\end{equation}

The condition that makes the OLS estimator unbiased is:
\begin{equation}
{\rm E} \bf (u|X)=0,
\label{exog}
\end{equation}

that is, the explanatory variables which form the columns of $\bf X$
are exogenous.  This condition is weaker than the independence
condition that $u$ and $X$ are independent.

There is a weaker condition: 
\begin{equation}
{\rm E} \bf (X u)=0,
\label{exog2}
\end{equation}

From equation \ref{exog} we can derive equation \ref{exog2}
:\begin{equation}
{\rm E} \bf (X u)= {\rm E}( {\rm E} {\bf (X u | X)})= 0,
\end{equation}

Equation \ref{exog} says that given $\bf X$, the expected value of
$\bf u$ is zero; it implies that the model is correctly specified.
That is, $\bf y$ is a linear function of $\bf X$.  Condition
\ref{exog} is needed for unbiasedness.  Once we know that the model is
correctly specified, then equation \ref{exog2} can be used to derive
results, such as GMM estimators.

In the context of cross-sectional data, this assumption is plausible.
However, when we have time series data, the assumption becomes strong,
because it assumes that the entire series of $\bf X$ has no
relationship with the error term.  In a time series context, this is
hard to satisfy.  The OLS estimator is biased if condition \ref{exog}
is not satisfied.

For example, suppose we have a model  

\[ y_t = \beta_1 + \beta_2 y_{t-1} + u_t, \quad u_t \sim {\rm IID} (0, \sigma^2). \]

In this simple model, even if we assume that $y_{t-1}$ and $u_t$ are
uncorrelated, OLS estimator is still biased.  That is because
condition \ref{exog} is not satisfied: $y_{t-1}$ depends on $u_{t-1}$,
$u_{t-2}$ and so on.  Assumption \ref{exog} does not hold for
regressions with lagged dependent variables.  Models with time series
data are likely to violate assumption \ref{exog}.


\subsection{When is OLS estimator consistent?}

For OLS estimator to be consistent, a much weaker condition is needed: 
\begin{equation}
{\rm E} (u_t|X_t)=0,
\label{pred}
\end{equation}

This condition is much weaker since it only assumes that the mean of
current error term does not depend on the current predictors.  Even a
model with lagged dependent variable can easily satisfy this
condition.  Condition \ref{pred} is called predeterminedness
condition, or say regressors are predetermined.


\section{Estimation of Variance}

If the OLS estimator is unbiased (that is, if $\bf X$ are exogenous),
and suppose that data is generated by this model:

the linear regression models is

\begin{equation}
\bf y=X\beta+u, \quad {\rm E} (u | X)=0, \quad {\rm E} (u u')=\Omega,
\end{equation}


that is, the error terms are independently but not identically
distributed, then the following holds:

\begin{eqnarray}
{\rm Var} \bf (\hat \beta) & = & {\rm E}\bf [(\hat \beta
-\beta)(\hat
\beta -\beta)'] \\
& = & \bf [(X'X)^{-1}X'({\rm E}(uu'))X(X'X)^{-1}] \nonumber \\
& = & \bf  (X'X)^{-1} X' \Omega X (X'X)^{-1} \nonumber
\end{eqnarray}

In the case of IID, $\Omega$ is identity matrix, and 

\begin{eqnarray}
{\rm var} \bf (\hat \beta)  =  \bf  \sigma^2 (X'X)^{-1} \nonumber
\end{eqnarray}

In more general situations, the error terms are not IID.  


\subsection{White's estimator}

In the case of heteroskedatic errors, $\bf \Omega$ is the error
variance-covariance matrix with diagonal elements being $\sigma_t^2$
for $t^{th}$ element, off-diagonal elements being zero.

If we know $\sigma_t^2$, then we would be able to estimate this
"sandwich covariance matrix".  But we don't.
\begin{eqnarray}
{\rm Var} \bf (\hat \beta) & = & \bf \frac{1}{n}
[\frac{1}{n}(X'X)]^{-1}[\frac{1}{n}X'\Omega
X][\frac{1}{n}(X'X)]^{-1} \nonumber
\end{eqnarray}

Let $y_t$ denote the $t$th observation on the dependent variable,
and $x_t'=[1 \ x_{2t} \  \cdots  \ x_{kt}]$ denote the $t$th row of the
$\bf X$ matrix.  Then
\begin{equation}
\bf X' \Omega X=\sum_{t=1}^n \sigma_t^2 x_t x_t'
\end{equation}
There are $n$ distinct $\sigma_t^2$ to estimate, the problem seems
hopeless: with $n$ goes to infinity, the number of parameters need to
estimate also goes to infinity.  White (1980) shows that we don't need
to do that: all we need to do is to get a consistent estimator for
$\bf X' \Omega X$, which is $k \times k$ and symmetric.  It has
$\frac{1}{2} (k^2+k)$ distinct elements.  The White estimator replaces
the unknown $\sigma_t^2$ by $\hat u_t^2$, the estimated OLS residuals.
This provides a consistent estimator of the variance matrix for the
OLS coefficient vector and is particularly useful since it does not
require any specific assumptions about the form of the
heteroscedasticity.  This type of estimator is also called
heteroskedasticity-consistent covariance matrix estimator.

\begin{eqnarray}
\hat {\rm Var} \bf (\hat \beta) & = & \bf \frac{1}{n}
[\frac{1}{n}(X'X)]^{-1}[\frac{1}{n}X' \hat \Omega
X][\frac{1}{n}(X'X)]^{-1}\\
\bf \hat \Omega & = & \rm{diag}  ({\hat u_1^2, \hat u_2^2, \cdots,
\hat u_n^2})
\end{eqnarray}


\subsection{Newey-West estimator}

White's estimator deals with the situation that we have
heteroskedasticity (a diagonal $\Sigma$) of unknown form.  When we
have serial correlation of unknown form (a non-diagonal $\Sigma$), we
can estimate the variance-covariance matrix by a heteroskedasticity
and autocorrelation consistent, or HAC, estimator.  Newey-West
estimator is the most popular HAC estimator.  It's not as
straightforward as White's estimator to illustrate, but I'll try to
summarize.

\subsubsection{ Consistent Estimation of the Variance of the Sample Mean}

Given a time series data set, suppose we are interested in estimating
the mean vector (suppose we have more than one variable) and its
variance.  We know that given IID data, we can apply central limit
theorem: sample mean is a consistent estimator of the population mean
and it's variance can be calculated since asymptotically the sample
mean conforms to a normal distribution and the variance can be
estimated, relatively easily.  However, in the case of time series
data, autocorrelation usually exists.  We may be concerned the CLT may
not work in this case.

Fortunately, as proved in Hamilton (1994), if $\bf y_t$ is a
covariance-stationary (meaning that the covariance is not a function
of time) vector process, then the sample mean satisfies:
\begin{enumerate}
\item \[\bf \bar y_t \to \mu , \]
\item \[ {\bf S} = \lim_{T \to \infty} \bf  {T \cdot E[(\bar y_T - \mu)(\bar y_T -\mu)' ]} = \sum_{v=-\infty}^{\infty} \Gamma_v . \]
where $\bf \Gamma_v$ is the variance-covariance matrix for $\bf y_t$ and $\bf y_{t-v}$.  
\end{enumerate}

The first one says for a covariance-stationary vector process, the law
of large numbers still holds.  The second one is used to calculate the
standard error.

If the data were generated by a vector MA(q) process, then 

 \[ {\bf S} = \sum_{v=-q}^{q} \Gamma_v . \]

A natural estimate is

 \[  \bf \hat S = \hat \Gamma_0 + \sum_{v=1}^{q} (\hat \Gamma_v + \hat \Gamma_v' ), \]

where  \[  \bf \hat \Gamma_v = (1/T)  \sum_{t=v+1}^{T} (y_t- \bar y)(y_{t-v} - \bar y). \]

This gives a consistent estimate of $\bf S$; however, it sometimes is
not positive semidefinite.

Newey-West (1987) suggested putting in a weight:

 \[  {\bf \hat S = \hat \Gamma_0} + \sum_{v=1}^{q} {(1- \frac{v}{q+1})} (\bf \hat \Gamma_v + \hat \Gamma_v' ), \]
where $q$ is from the MA($q$) process.


\subsubsection{ Newey-West estimator for linear regressions}

Consider a linear regression model:
\[ y_t={ \bf x_t' \beta} + u_t \]

Suppose we have the OLS estimator $\bf b_T$, then
\[ \sqrt{T} ({\bf b_t - \beta}) = [(1/T) {\sum_{t=1}^T \bf x_t
  x_t'}]^{-1} [({\sqrt{T}}) {\sum_{t=1}^{T} \bf x_t u_t'}] \] The
first term converges in probability to some constant.  The second term
is the sample mean of the vector $\bf x_t u_t$.

Under general conditions, 

\[ \sqrt{T} ({\bf b_t - \beta}) \rightarrow^L N(0, Q^{-1}SQ^{-1}) \] 

where $S$ can be estimated by
 \[  {\bf \hat S_T = \hat \Gamma_{0T}} + \sum_{v=1}^{q} {(1- \frac{v}{q+1})} (\bf \hat \Gamma_{v,T} + \hat \Gamma_{v,T}' ), \]
where 
\[ \hat \Gamma_{v,T}'= (1/T) \sum_{t=v+1}^T (x_t \hat u_{t, T} \hat u_{t-v, T} x_{t-v}'), \]
where $\hat u_{t,T}$ is the OLS residual for data $t$ in a sample of size $T$.

Overall, the variance of $\bf b_T$ is approximated by

\[\hat \Sigma_{NW} = [\sum_{t=1}^T x_t x_t'] [ \sum_{t=1}^T \hat u_t^2 x_t x_t' +  \sum_{v=1}^{q} {(1- \frac{v}{q+1})} \sum_{t=v+1}^T (x_t \hat u_{t, T} \hat u_{t-v, T} x_{t-v}' + x_{t-v} \hat u_{t-v, T} \hat u_{t, T} x_{t}') ] [\sum_{t=1}^T x_t x_t']^{-1} \]


This estimation obviously depends on the selection of $q$, the lag
length beyond which we are willing to assume that the autocorrelation
of $x_t u_t$ and $x_{t-v} u_{t-v}$ is essentially zero.  The rule of
thumb for the selection of $q$ is $0.75 \cdot T^{\frac{1}{3}}$.
Newey-West (1994) has suggested a way to automatically select the
bandwidth $q$.  Here we omitted the discussion.  Both Stata and R now
also implement Newey-West (1994) estimator, with no need to specify
$q$.

\subsubsection{ Implementation}

Stata has $newey$ and $newey2$ implemented for cross-sectional data.
For panel data, it has $xtivreg2$ which implements Newey-West (1994)
estimator with automatic bandwidth selection.

R has a library called $sandwich$ which implements different $HAC$
estimators, including Newey-West.


\section{Comparing the three variance estimators: OLS, robust, and robust cluster (from Stata website)}
The formulas for the estimators are

\begin{enumerate}

\item OLS variance estimator:
\[V_{OLS} = s^2 * (X'X)^{-1} \]
where
\[s^2 = (1/(N - k)) \sum_{i=1}^{N} e_i^2\]

\item robust (unclustered) variance estimator:
\[V_{robust} = (X'X)^{-1} * [ \sum_{i=1}^{N}(e_i*x_i)' * (e_i*x_i) ] * (X'X)^{-1}\]
\item robust cluster variance estimator:
\[V_{cluster} = (X'X)^{-1} * (\sum_{i=1}^{n_c} u_j'*u_j)* (X'X)^{-1}\]
where \[ u_j = \sum e_i*x_i\]  and $n_c$ is the total number of clusters.
\end{enumerate}

Above, $e_i$ is the residual for the $i$th observation and $x_i$ is a row
vector of predictors including the constant.

Interpreting a difference between (2) the robust (unclustered) estimator and
(3) the robust cluster estimator is straightforward. If the variance of the
clustered estimator is less than the robust (unclustered) estimator, it means
that the cluster sums of ei*xi have less variability than the individual ei*xi.
That is, when you sum the ei*xi within a cluster, some of the variation gets
canceled out, and the total variation is less. This means a big positive is
summed with a big negative to produce something small  in other words, there
is negative correlation within cluster.

Interpreting a difference between (1) the OLS estimator and (2) or (3) is
trickier. In (1) the squared residuals are summed, but in (2) and (3) the
residuals are multiplied by the x's (then for (3) summed within cluster) and
then "squared" and summed. Hence, any difference between them has to do with
correlations between the residuals and the x's. If big (in absolute value) ei
are paired with big xi, then the robust variance estimate will be bigger than
the OLS estimate. If, on the other hand, the robust variance estimate is
smaller than the OLS estimate, what's happening is not clear at all, but has to
do with some odd correlations between the residuals and the x's.

Note that if the OLS model is true, the residuals should, of course, be
uncorrelated with the x's. Indeed, if all the assumptions of the OLS model are
true, then the expected values of (1) the OLS estimator and (2) the robust
(unclustered) estimator are approximately the same when the default multiplier
is used.
