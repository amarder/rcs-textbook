\chapter{Introduction to Endogeneity-related Methods}

\section{Endogeneity}

When we discuss the linear model we assume that the regressors are
exogenous, meaning that they are independent of or uncorrelated with
the error term.  Often there are reasons to believe that some
regressors are correlated with the error term.  In that case we call
those regressors endogenous.


Under the classical assumptions OLS estimators are Best Linear
Unbiased Estimator (BLUE).  One key assumption is that the regressors
have to be uncorrelated with the error term.  If this condition does
not hold, OLS estimators are biased and inconsistent.

When one independent variable does not satisfy this condition, we say
this variable is endogenous.

The most popular cure for endogeneity is to use instrumental variables.

\section{Instrumental Variables}

Suppose the linear regression model
\begin{equation}
\bf y=X\beta+u, \quad {\rm E} (u u')=\sigma^2 I, \label{eqn:iv}
\end{equation}
 at least one of the explanatory variables in the $n \times k$
matrix $\bf X$ is assumed not to be predetermined with respect to the
error terms, or say, endogenous.


If we have only one endogenous variable, the moment condition is
\begin{equation}
\bf Z'(y-X\beta)=0.
\end{equation}

since there are $k$ equations and $k$ unknowns, we can solve it to
obtain the simple IV estimator
\begin{equation}
\bf \hat \beta_{IV} \equiv (Z'X)^{-1}Z'y.
\end{equation}

Suppose we have a set of  variables $\bf Z$, an $n \times l$
matrix of instruments , which satisfies the moment
condition
\begin{equation}
\bf Z'(y-X\beta)=0.
\end{equation}

Here is what we do for two-stage least squares (2sls): 

Stage 1:  Regress each of the variables in the $\bf X$ matrix on
$\bf Z$ to obtain a matrix of fitted values $\bf \hat X$,

\begin{equation}
\bf \hat X=Z(Z'Z)^{-1}Z'X=P_ZX
\end{equation}

Stage 2:  Regress $\bf y$ on $\bf \hat X$ to obtain the estimated
$\bf \beta$

\begin{equation}
\bf  \hat \beta_{2sls}=(\hat X' \hat X)^{-1}(\hat X'
y)=(X'P_ZX)^{-1}(X'P_Zy)=\hat \beta_{IV}
\end{equation}

Standard errors:

\begin{equation}
Var \bf [ \hat \beta_{2sls}]= \hat \sigma^2 (\hat X' \hat X)^{-1}= \hat
\sigma^2 (X'P_ZX)^{-1}
\end{equation}
where $ \hat \sigma^2 ={\bf \hat u' \hat u} / N$.  Note that $\bf \hat
u = y- X \hat \beta_{2sls}$.

If we do it manually by two steps, the second step will report a wrong
standard error.  That is because the second stage regression will
report standard errors based on $\bf \hat u = y- \hat X \hat
\beta_{2sls}$.  Therefore, it's always recommanded to ask the
statistical program to do a 2sls for you, since presumably that will
give you correct standard errors.

Geometry Illustration:

Suppose the simplest case:  $y=\beta_0 + \beta_ x + u $, where $x$
can be decomposed into $x_1$, which is exogenous and $x_2$, which
is endogenous. Therefore $x_2$ is parallel to $u$ and $x_1$
perpendicular to $u$.  Suppose $z$ is a vector that is
perpendicular to $x_2$ or $u$, but not perpendicular to $x_1$.
Then $z$ can be an instrument for $x$.  The way instrumental
variable works: Regress $x$ on $z$, suppose $\hat x_1$ is the
projection.  Regress $y$ on $z$, $\hat y_2$ be the projection.
Then the result of $\beta_2 =  \hat y_2 / \hat x_1$ is the same as
$\beta_1= \hat y_1 / x_1$ (since the two triangles are similar).
Here $\hat y_1$ is the projection of $y$ on $x_1$, which is
hypothetical since we have no way to decompose $x$ into $x_1$ and
$x_2$; otherwise, we would not need instrumental variables.


% \begin{figure}[htbp] \footnotesize \caption{\small Instrumental Variable geometry illustration}
% \centerline{\includegraphics[width=6in,height=6in]{iv.eps}}
% \label{fig:iv}
% \end{figure}


\subsection{Implementation in Stata}

To do 2sls, in Stata 10 or 11, use ``ivregress 2sls'' command.  

\subsection{Control Function Approach}

A second way to do an IV regression is also two-step approach: Regress
$\bf X$ (endogenous) on $\bf Z$, get the residual: $\bf \hat
v=X-Z(Z'Z)^{-1}Z'X$, then regress $y$ on $\bf X$ and $\hat v$ to get
$\hat \beta_{IV}$.

The difference between this approach and the 2sls approach is that in
2sls we regress $y$ on $\bf \hat X$; in the control function approach,
we regress $y$ on $\bf  X$ and $\hat v$.  They should both give you
the same coefficient estimates.  There are advantages of using the
control function approach.

First, you can do a simple test of endogeneity of $\bf X$.  For
example:

\begin{verbatim}
reg x_endog x* z*
predict v_x, resid
reg y x_endog x* v_x
test v_x
\end{verbatim}

Obviously, this test is based on $\bf Z$ being exogenous.

Secondly, it works for some non-linear models, such as logit, probit,
poisson, or any other glm models.  That is, if you have an endogenous
variable in a glm model, you can regress that endogenous variable on
instruments, get the residual, then run the glm model with the
original regressors, plus the residual from the first stage.

\section{Durbin-Wu-Hausman Test}

\subsection{Idea}

In econometric modeling, there are often questions on endogeneity.  Do
we know how to test whether an independent variable is endogenous
statistically?  The answer is: sort of, but not really.  We cannot do
endogeneity test without a valid instrument.  Therefore, we have to
have strong argument for a valid instrument first before we can do
endogeneity test.


With endogenous variables on the right-hand side of the equation, we
need to use instrumental variable (IV) regression for consistent
estimation.  However, with IV regression, we lose efficiency: the
asymptotic variance of the IV estimator is larger, and can be much
larger than the OLS estimator.  Therefore, we gain consistency, but
lose efficiency, by using IV estimator when there is an endogeneity
problem.  

Now we have a familiar scenario (if you are familiar with Hausman
test for fixed effect and random effect estimator for panel data):
Suppose we have the null hypothesis as the regressor being exogenous.
We have an efficient estimator under null hypothesis yet inconsistent
under alternative hypothesis (OLS estimator). We also have a
consistent estimator under both null and alternative (IV estimator).

Similar to panel data setting, we have the Hausman test statistic as:

\[ H = (\hat \beta_c - \hat \beta_e)' D^{-} (\hat \beta_c - \hat
\beta_e) \]

where $D={\rm Var} [\hat \beta_c] - {\rm Var} [\hat \beta_e]$, $^-$ is
the generalized inverse, $\hat \beta_c$ is the consistent estimator (in
this case the IV estimator) and $\hat \beta_e$ is the efficient
estimator (in this case OLS estimator).

$H$ conforms to $\chi^2_k$ asymptotically, where $k$ is the number of endogenous
variables.

This test is to compare the IV estimator and the OLS estimator: if
it's close, then OLS estimator is fine (fail to reject null that OLS
is consistent, or say the variable is exogenous).  If it's large, then
IV estimator is needed, although we lose some efficiency.  This test
is also based on the assumption that the instruments are exogenous.
If that is in question, then it's pointless to do the test, since the
IV estimator cannot guarantee consistency either.



\subsection{Implementation in Stata}

In Stata, there are different ways to do it:

\begin{enumerate}

\item Do a regular Hausman test:
\begin{itemize}
\item ivreg y x1 (x2=x3 x4)
\item estimates store iv
\item reg y x1 x2
\item hausman iv ., constant sigmamore
\end{itemize}

\item Use ivendog

\end{enumerate}

\section{Identification}

Identification in a regression equation means that all parameters can
be uniquely estimated.  A necessary condition of that is to have at
least as many instruments as the number of endogenous variables.  That
is, $l \ge k$ in our example above.  If $l=k$ we have
exact-identification.  If $l>k$, we have over-identification.  

Over-identification generates more efficient estimates, given the
assumption of instruments being exogenous.  The other advantage of
over-identification is that over-identification tests can be done to
test the adequacy of instruments.

Under the null hypothesis that all the instruments are uncorrelated
with the error term, an LM statistic  $N \times R^2$ conforms to
$\chi^2 (r)$ distribution, $r=l-k$, the number of excess instruments,
 or say, the number of excluded restrictions.  If we reject the null,
 then we should be concerned about the exogeneity of the whole set of
 the instruments.  This test is called Sargan's test in IV context, and
 (Hansen's) J test in GMM context.  

 What the J test or Sargan's test does is to test the whole set of
 instruments being exogenous or not.  There is another test for
 testing exogeneity for a subset of instruments.  It's call a C test
 or a difference-in-Sargan test.  The idea is to calculate the
 difference between two Sargan's statistics (or Hansen's J in GMM
 setting); one is with the whole set of instruments, the other one
 without the suspected instruments.  The null is that the suspect
 instruments are exogenous; or orthogonal to the error term.
 Obviously to conduct the C test, we'll have to have at least one
 extra instrument more than the number of endogenous variables.

 To understand it better, we look at how to implement Sargan's test
 manually: For the 2SLS estimator, the test statistic is Sargan's
 statistic, typically calculated as $N \times R^2$ from a regression of
 the IV residuals on the full set of instruments.

\begin{verbatim}
. ivregress 2sls rent pcturban (hsngval = faminc i.region)
. estat overid

  Tests of overidentifying restrictions:

  Sargan (score) chi2(3) =  11.2877  (p = 0.0103)
  Basmann chi2(3)        =  12.8294  (p = 0.0050)

. predict res, residual

. reg res pcturban faminc i.region

. disp e(N)*e(r2)
11.287665
\end{verbatim}




\subsection{Implementation in Stata}

In Stata, there are different ways to do over-identification test, {\em
  ivreg2} reports a comprehensive set of tests; {\em overid} command
does the over-identification test after the {\em ivreg} command.  

{\em  ivreg2} with {\em  gmm} option returns J test; it reports
Sargan's test without this option.

{\em  ivreg2} also reports C test statistic, with {\em  ortho()}.  If
the C test rejects the null, and J test without the suspect
instruments fail to reject null, then the suspect instruments are
indeed the ones are not exogenous.



\section{Weak Instruments}

\subsection{Problem with the cure}

An instrument needs to satisfy to criteria: orthogonality and
relevance.  We need instruments to be orthogonal to the error term.
We can verify the orthogonality condition  by Sargan's test if there are extra
instruments.  

It turns out instrument relevance is important too: if instruments are
weak, then the regular large sample properties of IV or GMM estimators
do not hold any more.  The estimators are inconsistent or biased.

To see the problem, suppose 

\begin{equation}
\bf y=X\beta+u, \quad {\rm E} (u u')=\sigma_u^2 I, \label{eqn:wi1}
\end{equation}

\begin{equation}
\bf X=Z\Pi+v, \quad {\rm E} (v v')=\sigma_v^2 I, \label{eqn:wi2}
\end{equation}

and 
\begin{equation}
{\rm E}(\bf Z u )=0. \label{eqn:zu}
\end{equation}

We can see here $\bf Z$ is exogenous.  However, the model does not say
anything about relevance.  To illustrate the problem caused by weak
instrument, suppose we have only one endogenous variable and one
instrument.  

\begin{equation}
\hat \beta_{2sls}=\frac{\bf Z' y}{\bf Z' X} = \frac{\bf Z'(X \beta +
  u)}{\bf Z'
  X}= \beta + \frac{\bf Z' u}{\bf Z' X} . \label{eqn:wk3}
\end{equation}

If $\bf Z$ is irrelevant, or, $\Pi=0$, then

\begin{equation}
\hat \beta_{2sls}- \beta=\frac{\bf Z' u}{\bf Z' v} =\frac{ \frac{1}{\sqrt n}
\sum_{i=1}^N Z_i u_i}{\frac{1}{\sqrt n} \sum_{i=1}^N Z_i v_i}
\xrightarrow{d} \frac{z_u}{z_v} ,
\end{equation}

where

\begin{equation}
\begin{bmatrix}
z_u \\z_v
\end{bmatrix}
\sim N(0, \begin{bmatrix}
\sigma_u^2 \quad \sigma_{uv} \\  \sigma_{uv} \quad \sigma_v^2
\end{bmatrix}).
\end{equation}

Therefore, if $\bf Z$ is irrelavent, $\beta_{2sls}$ is inconsistent.
Also, the distribution of the bias is Cauchy-like (the ratio of
correlated normals).

This is a case where the cure might be worse than the disease itself:
the bias can be big comparing to the bias an OLS estimate suffers.

\subsection{Tests}

There are a variety of weak-instruments tests proposed.  Most of them
are based on so-called weak-instruments asymptotics and a new
parameter called {\em concentration parameter} $\mu^2 = \Pi' Z' Z \Pi
/ \sigma_v^2$.  Sample size only enters the distribution through
$\mu^2$.

With weak-instruments asymptotics, IV estimators are no longer
consistent, and they are not normal asymptotically.  Most test
statistics (J test, etc.) do not have normal or $\chi^2$ distributions
anymore.

Now I list the following tests in the order of recommended level by
James Stock:

\begin{enumerate}

\item Moreira (2003) conditional likelihood ratio test (CLR).


  Advantages of this test: \begin{enumerate}
\item Uniformly most powerful tests among valid tests.  \item
  Implemented in Stata as {\em condivereg}.  \end{enumerate}

Disadvantages: 
 \begin{enumerate}
\item Complicated.  \item  Only developed so far for one endogenous
  variable case.  \end{enumerate}

\item Stock-Yogo bias method and size method.  

Stock and Yogo (2005) provide critical values for both methods: one is
to control the size of bias, the other one is to control the size of a
Wald test of $\beta=\beta_0$.  Bias method is more frequently used.
In the case of multiple endogenous variables, the Craigg-Donald
statistics is used to compare with the critical values.  It is
implemented in Stata as part of the {\em ivreg2} command, but it's
only available for the situation there are at least two excluded
variables (meaning the number of instruments minus the number of
endogenous variables).  

\item  Anderson-Rubin confidence intervals.  

In the model of 

\begin{equation}
\bf y=X\beta+u, \quad {\rm E} (u u')=\sigma_u^2 I,
\end{equation}

\begin{equation}
\bf X=Z\Pi+v, \quad {\rm E} (v v')=\sigma_v^2 I,
\end{equation}

The null hypothesis $H_0: \beta=\beta_0$.  Anderson-Rubin statistic
is the F statistic in the regression of $y-X \beta_0$ on $Z$, the F
test on $\Pi$ being zero:


\begin{equation}
AR(\beta_0)= \frac{(y-X\beta_0)' P_Z (y-X\beta_0)/k}{(y-X\beta_0)' M_Z
  (y-X\beta_0)/(N-k)}
\end{equation}

The idea of AR confidence interval is to construct an interval for all
possible values of $\beta$ to fail to reject $\Pi=0$. 

\item First-stage F test.

The rule of thumb for first-stage F test is $F>10$ for a single
instrument case, the more instruments, the higher it gets.

\item Kleibergen's LM test.

This test is dominated by the CLR test, thus no longer the optimal
test to use.

\item First-stage $R^2$, or partial $R^2$, etc., are not recommended.

\end{enumerate}


\section{When is endogeneity a problem and what can we say from an IV regression}

In an example by Bill Simpson,   a model of housing price on no. of rooms
and square footage.

\[ P = \alpha R + \beta F + \epsilon \]  

Should square footage really be there?  We are not interested in,
given square footage, how much money we need to pay for an additional
room.  We are generally interested in how much money we need to pay
for an additional footage.  However, if we do not include footage,
then we ``sort of'' have endogeneity problem due to omitted variable.  

Another example would be education's impact on wage.  Say IQ is an
omitted variable.  Do we want to say something about education's
impact on wage controlling on IQ, or not?

So it depends on what questions to ask.  If I am a builder and ask
what an extra bedroom on the already built house would bring to me, I
need to control for footage.  If I am generally asking what an extra
bedroom would cost, then I am implying what that bedroom and
associated footage would cost me.  In that case, without controlling
for footage is not a problem, since the question is:  what an
additional bedroom (AND the associated average square footage) would
cost me.

In the wage and education example, if I am asking: what's the effect
of education on wage?  I implicitly ask what would be my pay raise if
I go to college (given IQ, of course), for example.  Then we are
concerned that this model of
\[ wage= \alpha edu + \epsilon \] 
will generate biased estimate of $\alpha$, because we are not
controlling for IQ.  But if I am an employer and interested in hiring
a person, what would I have to pay extra to hire a collage graduate
vs. a high school graduate?  Then I don't have to include that IQ
variable, because implicitly I am asking: what extra I need to pay for
that college graduate who comes with a higher IQ, and everything else
that is associated with higher education.

In addition, what an IV regression really gives us is part of the causal
inference of $X$ (suspected to be endogenous) on $y$, namely the part
induced by $Z$.  For example, we are interested in the relationship
between colleage education and wage.  If we use distance to college as
an instrument, then our inference is the effect of college education
from those who decide to go because of nearbyness of the college.



\begin{thebibliography}{99}

\bibitem{Woodridge}Woodridge, J.
 \newblock (2001)
 \newblock {\em Econometric Analysis of Cross Section and Panel Data}
 \newblock The MIT Press


\bibitem{Baum}Baum, K
 \newblock (2006)
 \newblock {\em An Introduction to Modern Econometrics using Stata}
 \newblock Stata Press


\end{thebibliography}
