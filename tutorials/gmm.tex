\chapter{Generalized Method of Moments}


\section{The Method of Moments}

The method of moments (MOM) is merely the following proposal:

\begin{prop}{\bf{(MOM)}}

To estimate a population moment (or a function of population
moments) merely use the corresponding sample moment (or a function
of sample moments).
\end{prop}

A population moment $\gamma$ can be defined as the expectation of
some continuous function $g$ of a random variable $x$:
\begin{equation}
\gamma={\mathrm{E}} [g(x)]
\end{equation}

On the other hand, a sample moment is the sample version of the
population moment in a particular sample:
\begin{equation}
\hat \gamma=\frac{1}{n} \sum [g(x)]
\end{equation}

\section{OLS as a moment problem}

Consider the simple linear regression
\begin{equation}
\bf y=X\beta+u, \quad u \sim IID(0, \sigma^2).
\end{equation}

If the model is correctly specified, then
\begin{equation}
\rm E (\bf X'u)=0.
\end{equation}

The MOM principle suggests that we replace the left-hand side with
its sample analog $\frac{1}{n} \bf X'(y-X\beta)$.

Since we know that the true $\bf \beta$ sets the population moment
equal to zero in expectation, it seems reasonable to assume that a
good choice of $\bf \hat \beta$ would be one that sets the sample
moment to zero.  The MOM procedure suggests an estimate of $\bf
\beta$ that solves
\begin{equation}
\frac{1}{n} \bf X'(y-X \hat \beta)=0.
\end{equation}

The MOM estimator is
\begin{equation}
\bf \hat \beta=(X'X)^{-1}X'y,
\end{equation}
which is the same as the OLS estimator.

\section{IV as a moment problem}
Consider the simple linear regression
\begin{equation}
\bf y=X\beta+u, \quad u \sim IID(0, \sigma^2).
\end{equation}

If the model is mis-specified, then
\begin{equation}
\rm E (\bf X'u)\neq 0.
\end{equation}

We have to find an instrumental variable $\bf Z$ which is
\begin{equation}
\rm E (\bf Z'u)= 0.
\end{equation}
Or,
\begin{equation}
\rm E (\bf Z'(y-X\beta))= 0.
\end{equation}

The sample analogy of this is
\begin{equation}
\frac{1}{n} \bf Z'(y-X \hat \beta)=0.
\end{equation}

That gives us the IV estimator
\begin{equation}
\bf \hat \beta=(Z'X)^{-1}Z'y.
\end{equation}

\section{The Generalized Method of Moments}

\subsection{Moments}

The expectation ${\rm E}(Y^r)$ for any $r=1,2, \dots$ is called the
$r^{th}$ (raw) moment of $Y$.  The expectation ${\rm E} [(Y-{\rm
  E}(Y))^r]$ is called the $r^{th}$ centered moment of $Y$.

The mean is the first raw moment.

The variance is the second centered moment.

The third centered moment measures the skewness of the
distribution.

The fourth centered moment measures the kurtosis of the
distribution.  Interpreted as a measure of "fatness of tails".

The standardized kurtosis is
\[k=\frac{E[(Y-E(Y))^4]}{E[(Y-E(Y))^2]^2}.\]

For a normal distribution, $k=3.$

For a $t$ distribution with $v \geq 5$ degrees of freedom,
$k=3+6/(v-4) > 4.$  i.e., the $t$ distribution has fatter tails
than a normal distribution.

The distribution function of a random variable captures all
information about the random variable.  It can be shown using all
moments also captures all information.

This distinction underlies the relative strengths and weaknesses
of ML and GMM.

\subsection{GMM}

The statistical model takes the general form
\begin{equation}
E[m(Y_i; \theta_0)]=0
\end{equation}
where
\begin{itemize}
\item $Y_1, \cdots, Y_2$ are random variables from which the
sample $y_1, \cdots, y_n$ is drawn, \item $m(Y, \theta)$ is a
function specifying the model, \item $\theta_0$ is the "true
value" of the parameter.
\end{itemize}

$E[m(Y_i; \theta_0)]=0$ are called the population moment
conditions.\\

Two ideas behind GMM:
\begin{enumerate}
\item Replace the population mean $E[.]$ with the sample mean
  calculated from the observed sample $y_1, \cdots, y_n$. \item Since
  $E[m(Y_i; \theta_0)]=0$, choose $\hat \theta_{GMM}$ to make
  $\frac{1}{n}\sum_{i=1}^{n}m(y_i; \hat \theta_{GMM})$ as close to
  zero as possible.
\end{enumerate}

Define the notation

\begin{equation}
\bar m(\theta)=\frac{1}{n} \sum_{i=1}^n m(y_i; \theta).
\end{equation}

$\hat \theta_{GMM}$ is chosen to make $\bar m(\theta)'\bar
m(\theta)$ as close to
zero as possible.\\

More generally, $\hat \theta_{GMM}$ is chosen to minimize $\bar
m(\theta)'W \bar m(\theta)$ for some weighting matrix $W$.

\subsection{A GMM example}

Suppose we have people's income data, which are non-negative,
highly skewed and contain large outliers.

Consider the gamma distribution with pdf
\begin{equation}
f(y; \alpha, \beta)=\frac{y^{\alpha-1}
{\mathrm{exp}}(-y/\beta)}{\Gamma(\alpha)\beta^\alpha}, \ y,
\alpha, \beta>0.
\end{equation}

If $Y_i$ has a $\gamma(\alpha_0, \beta_0)$ distribution then
\begin{equation}
E(Y_i)=\frac{\alpha_0}{\beta_0}, \
E(Y_i^2)=\frac{\alpha_0+\alpha^2}{\beta_0^2}.
\end{equation}

So the two moment conditions are
\begin{equation}
E[m(Y_i; \theta_0)]=0
\end{equation}
where $\theta=(\alpha, \beta)$ and
\begin{equation}
m(Y_i; \theta)=
\begin{bmatrix}
Y_i-\alpha/\beta \\ Y_i^2-(\alpha+\alpha^2)/\beta^2
\end{bmatrix}
\end{equation}

The sample moment conditions are
\begin{equation}
\frac{1}{n} m(y_i; \hat \theta_{GMM})=0,
\end{equation}

i.e.
\begin{equation}
\begin{bmatrix}
\frac{1}{n} \sum_{i=1}^{n} y_i- \hat \alpha_{GMM}/ \hat \beta_{GMM} \\
\frac{1}{n} \sum_{i=1}^{n} y_i^2-(\hat \alpha_{GMM}+ \hat
\alpha_{GMM}^2) / \hat \beta_{GMM}^2
\end{bmatrix}
=0.
\end{equation}

The solution is
\begin{equation}
\begin{bmatrix}
\hat \alpha_{GMM} \\
\hat \beta_{GMM}
\end{bmatrix}
=\begin{bmatrix}
\frac{\bar y^2}{s^2} \\
\frac{\bar y}{s^2}
\end{bmatrix}.
\end{equation}

\subsection{Conditioning}

{\bf Independence}\\

If $X$ and $Y$ are independent then
\begin{equation}
f(x,y)=f(x)f(y)
\end{equation}
and hence
\begin{equation}
f(y|x)=f(y).
\end{equation}

If $X$ and $Y$ are independent then
\begin{equation}
E[g(X)h(Y)]=E[g(X)]\cdot E[h(Y)]
\end{equation}
and hence
\begin{equation}
Cov[g(X),h(Y)]=0.
\end{equation}
i.e. all functions of $X$ and $Y$ are uncorrelated.

{\bf Law of Iterated Expectations}\\
\begin{equation}
E[Y]=E[E(Y|X)].
\end{equation}

{\bf Dependence Concepts}\\

\begin{itemize}
\item $X$, $Y$ independent:
\begin{equation}
Cov[g(X),h(Y)]=0
\end{equation}
\item $X$, $Y$ uncorrelated:
\begin{equation}
Cov[X,Y]=0
\end{equation}
\item $E[Y|X]=0$:
\begin{equation}
Cov[g(X),Y]=0
\end{equation}
\end{itemize}

\subsection{Regression}

A regression model is a model of $E[Y_i|X_i]$.  For example,
\begin{equation}
Y_i=\beta_0+\beta_1X_i+u_i
\end{equation}
where $E[u_i|X_i]=0$.\\

{\bf GMM regression}\\

The regression model
\begin{equation}
Y_i=\beta_0+\beta_1X_i+u_i, \quad E[u_i|X_i]=0
\end{equation}
implies the moment condition
\begin{equation}
E[u_i]=0 \quad  \mbox{and} \quad  E[X_i u_i]=0
\end{equation}

That is,
\begin{equation}
E[Y_i-\beta_0-\beta_1X_i]=0
\end{equation}
\begin{equation}
E[X_i(Y_i-\beta_0-\beta_1X_i)]=0
\end{equation}

The sample moment conditions are
\begin{equation}
\frac{1}{n}\sum_{i=1}{n}(y_i-\hat \beta_0-\hat \beta_1x_i)=0
\end{equation}
\begin{equation}
\frac{1}{n}\sum_{i=1}{n}x_i(y_i-\hat \beta_0-\hat \beta_1x_i)=0
\end{equation}

These are just normal equations for OLS.

A characteristic of GMM:  the specification of the model generates
the estimator.  i.e. only $E[Y_i|X_i]=\beta_0+\beta_1 X_i$ is
assumed.

Note there are no assumptions that $u_i$ is homoscedastic, not
autocorrelated or normally distributed.  These properties affect
the statistical properties of the GMM estimator, not its
definition.
