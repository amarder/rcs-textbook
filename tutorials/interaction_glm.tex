\chapter{Interaction Effects in Generalized Linear Models}

\section{Introduction}

In generalized linear models with a link function other than the
identity, the relationship between the explanatory variables and the
response variable is nonlinear.  An implication of this nonlinearity
is that the marginal effect of any explanatory variable is in general
a function of all the other explanatory variables.  However, it is
true that the coefficients are the partial derivatives of the latent
variable on corresponding independent variables.

If the model is $g(\mu)=\beta_0+\beta_1 x_1 + \beta_2 x_2$ (for
simplicity we assume there are only two independent variables).  Then
$\frac{\partial \mu}{ \partial x_1}=\beta_1 f'()$, where $f=g^{-1}$
(assuming there are no interaction terms involving $x_1$ and no terms
with higher powers of $x_1$).  The nonlinearity also implies that
second order derivatives are in general nonzero, $\frac{\partial^2
  \mu}{\partial x_1^2}=\beta_1^2 f''()$ .  The nonzero second
derivatives are an inherent feature of models where there is a
nonlinear relationship between the right-hand side linear form and the
response variable.  They also occur in linear models if the response
variable is transformed, such as $log(y)=X'\beta$.

In a linear model, a nonzero second derivative can only occur if there
is a second-order term in the model.  For example, in the model
$y=\beta_0+\beta_1 x_1 + \beta_{11} x_1^2 + \beta_2 x_2 + \beta_{12}
x_1x_2$, $\frac{\partial^2 E(y)}{\partial x_1 \partial
  x_2}=\beta_{12}$ , and $\frac{\partial^2 E(y)}{\partial
  x_1^2}=\beta_{11}$ , because the model explicitly specifies
quadratic and interaction terms.

The fact that second derivatives are in general nonzero in generalized
linear models has been discussed previously (Nagler, 1991; Huang \&
Shields, 2000; Ai \& Norton, 2003; Hoetker, 2007).  However, different
authors have different interpretations of this effect and different
recommendations for practice.

Nagler (1991, pg 1393) wrote that “Examining predicted probabilities
generated by a nonlinear model such as probit or logit may produce
spurious results when used to determine interactive effects between
two independent variables” and argued that the “proper” test for
interaction should be the presence of a significant coefficient on the
multiplicative term $x1*x2$.  This test can be explained in terms of an
underlying latent variable.

Huang \& Shields (2000, pg 81) seem to agree with this interpretation.
“Consequently, a ‘substantive interaction’ (Nagler 1991, p. 1397)
must be modeled by a statistically significant multiplicative term in
the equation.”  However, they go on to say: “We thus argue that
scholars should not evaluate interactive effects in logit and probit
analysis based solely on the ‘underlying linear model’ (Nagler,
1991, p. 1402) but explicitly incorporate the nonlinearity of these
models into their analysis and interpretation.”  They show how the
inherent nonlinearity can give different results for predicted
probabilities than for the latent variable.

Ai \& Norton (2003) extend this argument to claim that: “the
statistical significance of the interaction effect cannot be tested
with a simple t-test on the coefficient of the interaction
term $\beta_{12}$.”  They derive an expression for $\frac{\partial^2 \mu}{\partial x_1 \partial
  x_2}$ ,  in the probit model, which
can be easily extended to other link functions, and a standard error
using the delta method approximation.  Norton, Wang, \& Ai (2004)
provide a Stata program for calculating $\frac{\partial^2 \mu}{\partial x_1 \partial
  x_2}$  and plotting as well as $\beta_{12} F'()$.

More recently, the Ai \& Norton approach has been recommended by
Hoetker (2007) in an influential review article in Strategic
Management Journal.

Given this conflicting advice, it would be reasonable for applied
researchers to wonder what their approach should be.

One reason for the discrepancy between these different authors is that
they use the term “interaction effect” to mean different things.
Nagler distinguishes between the “interactive effect” that is
inherent in nonlinear models and the “interactive hypothesis” and
corresponding “substantive interactive effect” that is the result of
a nonzero coefficient on the product term in the model.  Huang \&
Shields generally use the term “built-in interaction” to refer to
the inherent effect of the nonlinear model, and interactive effect to
refer to the combination of built-in and substantive interaction.  Ai
\& Norton use “interaction effect” to refer to the crossed partial
derivative, and “interaction term” to refer to the x1*x2 term in the
model.

Because different authors refer to different meanings, it is useful to
have different terms to distinguish them.  I will call $\frac{\partial^2
g(\mu)}{\partial x_1 \partial x_2}$ the
substantive interaction effect (consistent with Nagler and Huang \&
Shields).  I will call  $\frac{\partial^2
\mu}{\partial x_1 \partial x_2}$ the (full interaction) 2nd order
marginal effect.



As pointed out by Ai \& Norton, statistical software that calculates
marginal effects typically reports the marginal effect of the
interaction term as $\frac{\partial f(\eta)}{\partial x_{12}}=\hat
\beta_{12} f'(\eta)$.  This corresponds to neither type of interaction
effects we discussed.

The null hypothesis is that the coefficient on the interaction term is
zero. The appropriate test for that null hypothesis is to divide the
coefficient by the standard error and calculate a z-statistic.  Ai \&
Norton’s null hypothesis is different for each observation and is
that the cross-partial second derivative is zero. Their hypothesis is
about the marginal effect at each observation. They
define “interaction effects” as this marginal effect.  However, an
alternate definition would be to define the interaction effect based
on the coefficient.


This interactive effect is assumed in the model specification.  This
suggests the hazards of drawing inferences about relationships among
in-dependent variables from probit estimates of predicted
probabilities (Nagler, 1991, pg 1395, first column).



Also, Ai \& Norton's interaction effect is different for every
distinct value of covariates.  Standard errors are calculated by delta
method.    Null hypothesis is different for every distinct value of covariates.

Some authors [Hoetker, 2007, page 336] argue that significance of the
interaction effect should be tested by using Ai \& Norton's
interaction effect and its standard error.

The null hypothesis does not correspond to any parameter values in the model.

Examining predicted probabilities generated by a nonlinear model such
as probit or logit may produce spurious results when used to determine
interactive effects between two independent variables.  (Nagler,
abstract).

Note that Nagler suggests looking at the underlying latent model,
Huang \& Shields suggest looking at the predicted probabilities.
Neither suggests hypothesis testing.

“Use the t statistic to test the hypothesis that the interaction
effect equals zero, for given x.”  (Ai \& Norton, page 125).



\section{Latent Variables in GLM}

\subsection{generalized linear models}

Let's look at the issue from perspective of the generalized linear
models (GLM) (Nelder and Wedderburn, 1972), the explanatory variables
affect the response through a linear predictor

\[\eta_i={\bf x}_i' \beta \]

where $\bf x$ are explanatory variables and $\beta$ is the coefficient
vector.  

The relationship between the expectation of the response and the
linear predictor is 

\[\mu_i=g^{-1} (\eta_i) \]

or

\[g(\mu_i)=\eta_i \]

where $g()$ is called the link function.  Common links include
indentity, reciprocal, logarithm, logit, probit (normal),
complementary log-log, etc.  


\subsection{latent variable models}

For most GLM's, we can also derive them from a latent variable
perspective.  For example, suppose the response variable is binary,
say to buy a TV or not, or vote yes to a taxation bill.  Suppose the
propensity of buying a TV, or voting yes is $y^*$,
which we don't observe.  It is modeled as a linear function of the exploratory
variables.  

\[y^*= X'\beta + \epsilon\]

where $\epsilon$ is an error term which conforms to normal
distribution (Probit) or logistic distribution (logit).  

Then $\beta_i=\frac{\partial Y^* }{\partial x_i}$; that is, the
coefficients are the partial derivatives of the latent variable on
corresponding independent variables.  


For count data models, such as Poisson model, we are looking at how
many time an event happens during an interval.  For example, number of
fish caught at a park.  If we model this as a poisson process, we are
also assuming the time to event follows an exponential distribution.
This is very similar to surival analysis using exponential
distribution to model time to event.  The assumption would be events
being inependent from each other.  Other count data models such as
negative binomial model do not assume that, neither does the Cox
proportional hazard model.


Muthen (1984) and Skrondal and Rabe-Hesketh (2004) have discussed
Generalized Latent Variable models (GLVM).  Under the framework of
GLVM, the latent variable is linear function of the exploratory
variables.  The coefficients are the partial derivative of the latent
variable on the corresponding exploratory variables.  When there is an
interaction term between $x_1$ and $x_2$, the coffienct on that
interaction term $x_{12}$ is the second partial derivative of the
latent variable with respect to $x_1$ and $x_2$.  The hypothesis of no
interaction should be conducted in the sense of no interaction effect
between $x_1$ and $x_2$ on the latent variable, not probability of 1
in the dichotomous model, or the expected value of counts in the count
data model.  The interaction effect
between $x_1$ and $x_2$ on the the probability of being 1 in the
dichotomous model or expected value of counts in count data model is a
non-linear function of $x_1$ and $x_2$ and other exploratory
variables, and will differ from observation to observation.
